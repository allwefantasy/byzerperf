
<p align="center">
  <picture>    
    <img alt="byzer-perf" src="https://github.com/allwefantasy/byzer-llm/blob/master/docs/source/assets/logos/logo.jpg" width=55%>
  </picture>
</p>

<h3 align="center">
Perf Tool For Byzer-LLM
</h3>

<p align="center">
| <a href="./README.md"><b>English</b></a> | <a href="./README-CN.md"><b>ä¸­æ–‡</b></a> |

</p>

---

*Latest News* ğŸ”¥

- [2024/03] Release Byzer-Perf 0.1.1
- [2024/02] Release Byzer-Perf 0.1.0

---


## Installation

```shell
# or https://gitcode.com/allwefantasy11/byzerperf.git
git clone https://github.com/allwefantasy/byzerperf
pip install -r requirements.txt
pip install -U byzerperf
```

We recommend that you use the environment configured for [Byzer-LLM](https://github.com/allwefantasy/byzer-llm) and then just install byzerperf like this:

```shell
pip install -U byzerperf
```


## Usage

Deploy the model by [Byzer-LLM](https://github.com/allwefantasy/byzer-llm). With the model deployed, you can use the following command to test the performance of the model:

```shell
cd byzerperf
python perf.py --results-dir ./result  --prompts-dir ./prompts --num-concurrent-requests 5 --model chat --template qwen
```

The above command will send 5 concurrent requests to the model and the result will be saved in the `./result` directory.
The parameter template now supports:

1. qwen
2. yi 
3. default (Style: User:xxxx \nAssistant:xxxx )
3. auto

Since Byzer-LLM supports SaaS model and roprietary model deployment, so you can test the performance of any SaaS model or proprietary model.

If you prefer to use the Python API, you can use the following code to test the performance of the model.

Suppose we want to test the performance of the chat model with [5,10,15,20] concurrent requests separately, and the model is deployed with 4 GPUs, quantization is int4, and the LLM size is qwen-72B.

```python

import os
import ray

ray.init(address="auto",namespace="default",ignore_reinit_error=True)   

from byzerperf.perf import ByzerLLMPerf

num_gpus = 4
quantization = "int4"
llm_size = "qwen1_5-72B"

for i in range(5,25,5):
    num_concurrent_requests = i
    result_dir = f"/home/byzerllm/projects/byzerperf/result-{num_concurrent_requests}-{llm_size}-{quantization}-{num_gpus}gpu"

    byzer_llm_perf = ByzerLLMPerf.create(
            model="chat",                        
            num_concurrent_requests=num_concurrent_requests,            
            results_dir=result_dir,                        
            prompts_dir="/home/byzerllm/projects/byzerperf/prompts",
            template="qwen",        
        )        

    byzer_llm_perf.run()
    
```

After running the above code, you will get the performance test result in the `result_dir` directory.

```python
 
import ray
from byzerllm.utils.client import ByzerLLM,Templates
from byzerperf.perf import ByzerLLMPerfExplains

ray.init(address="auto",namespace="default",ignore_reinit_error=True) 

result = []

num_gpus = 4
quantization = "int4"
llm_size = "qwen1_5-72B"

llm = ByzerLLM()
chat_model_name = "chat"               
llm.setup_template(chat_model_name,"auto") 
llm.setup_default_model_name(chat_model_name)

for i in range(5,25,5):
    num_concurrent_requests = i
    result_dir = f"/home/byzerllm/projects/byzerperf/result-{num_concurrent_requests}-{llm_size}-{quantization}-{num_gpus}gpu"    
    explains = ByzerLLMPerfExplains(llm,result_dir)
    t,context = explains.run()  
    print()
    print()
    title = f"==========num_concurrent_requests:{num_concurrent_requests} total_requests: 84=============" 
    print(context)
    print(title)
    print(t)
    result.append(title)
    result.append(t)
```

Here is the output:

```

==========num_concurrent_requests:5 total_requests: 84=============
åœ¨å¹³å‡è¾“å…¥tokené•¿åº¦ä¸º18.68çš„æƒ…å†µä¸‹ï¼Œä»è¯·æ±‚è¾“å…¥åˆ°æœåŠ¡å™¨è¿”å›ç¬¬ä¸€ä¸ªtokençš„å¹³å‡æ—¶é—´ä¸º174.79msã€‚

æœåŠ¡å™¨æ¯ä¸ªè¯·æ±‚å¹³å‡åå23.21 tokens/s

å®¢æˆ·ç«¯æ¯ç§’ç”Ÿæˆ97.78 tokens
æœåŠ¡å™¨æ¯ç§’ç”Ÿæˆ115.33 tokens


==========num_concurrent_requests:10 total_requests: 84=============
åœ¨å¹³å‡è¾“å…¥tokené•¿åº¦ä¸º18.68çš„æƒ…å†µä¸‹ï¼Œä»è¯·æ±‚è¾“å…¥åˆ°æœåŠ¡å™¨è¿”å›ç¬¬ä¸€ä¸ªtokençš„å¹³å‡æ—¶é—´ä¸º227.48msã€‚

æœåŠ¡å™¨æ¯ä¸ªè¯·æ±‚å¹³å‡åå14.93 tokens/s

å®¢æˆ·ç«¯æ¯ç§’ç”Ÿæˆ121.59 tokens
æœåŠ¡å™¨æ¯ç§’ç”Ÿæˆ145.34 tokens


==========num_concurrent_requests:15 total_requests: 84=============
åœ¨å¹³å‡è¾“å…¥tokené•¿åº¦ä¸º18.68çš„æƒ…å†µä¸‹ï¼Œä»è¯·æ±‚è¾“å…¥åˆ°æœåŠ¡å™¨è¿”å›ç¬¬ä¸€ä¸ªtokençš„å¹³å‡æ—¶é—´ä¸º313.24msã€‚

æœåŠ¡å™¨æ¯ä¸ªè¯·æ±‚å¹³å‡åå13.73 tokens/s

å®¢æˆ·ç«¯æ¯ç§’ç”Ÿæˆ166.85 tokens
æœåŠ¡å™¨æ¯ç§’ç”Ÿæˆ198.85 tokens


==========num_concurrent_requests:20 total_requests: 84=============
åœ¨å¹³å‡è¾“å…¥tokené•¿åº¦ä¸º18.68çš„æƒ…å†µä¸‹ï¼Œä»è¯·æ±‚è¾“å…¥åˆ°æœåŠ¡å™¨è¿”å›ç¬¬ä¸€ä¸ªtokençš„å¹³å‡æ—¶é—´ä¸º465.92msã€‚

æœåŠ¡å™¨æ¯ä¸ªè¯·æ±‚å¹³å‡åå10.90 tokens/s

å®¢æˆ·ç«¯æ¯ç§’ç”Ÿæˆ162.73 tokens
æœåŠ¡å™¨æ¯ç§’ç”Ÿæˆ202.48 tokens
```

We hope that we can get the best num_concurrent_requests value from the above output:

```python
s = "\n\n".join(result)
t = llm.chat_oai(conversations=[{
    "role":"user",
    "content":f'''
æœ‰ä¸Šä¸‹æ–‡å¦‚ä¸‹ï¼š

```
{s}
```
è¯·å‚è€ƒä¸Šé¢çš„å†…å®¹ï¼Œéšç€num_concurrent_requestsçš„å¢åŠ ï¼Œå…¶ä»–æŒ‡æ ‡çš„å˜åŒ–æƒ…å†µåšä¸ªæ€»ç»“ã€‚
å¦å¤–åˆ†ææœ€ä½³å¹¶å‘æ•°åº”è¯¥æ˜¯å¤šå°‘ï¼Ÿéšç€å¹¶å‘æ•°ä¸Šå‡ååä¹Ÿä¼šä¸Šå‡ï¼Œè€Œå•æ¬¡è¯·æ±‚åååˆ™ä¼šä¸‹é™ï¼Œè¯·æ‰¾åˆ°ä¸¤è€…çš„äº¤æ±‡ç‚¹ï¼Œç»™å‡ºäº¤æ±‡ç‚¹ã€‚
'''
}])

print(t[0].output)
```

The output:

```
éšç€`num_concurrent_requests`çš„å¢åŠ ï¼Œä»¥ä¸‹æŒ‡æ ‡å‘ç”Ÿäº†å˜åŒ–ï¼š

1. **å¹³å‡å“åº”æ—¶é—´**ï¼šéšç€å¹¶å‘è¯·æ±‚çš„å¢åŠ ï¼Œä»è¯·æ±‚è¾“å…¥åˆ°æœåŠ¡å™¨è¿”å›ç¬¬ä¸€ä¸ªtokençš„å¹³å‡æ—¶é—´ä¹Ÿåœ¨å¢åŠ ã€‚è¿™è¡¨æ˜æœåŠ¡å™¨å¤„ç†è¯·æ±‚çš„å‹åŠ›å¢å¤§ï¼Œå“åº”æ—¶é—´å˜æ…¢ã€‚å…·ä½“æ¥è¯´ï¼Œä»5ä¸ªå¹¶å‘è¯·æ±‚çš„174.79mså¢åŠ åˆ°20ä¸ªå¹¶å‘è¯·æ±‚çš„465.92msã€‚

2. **æœåŠ¡å™¨æ¯ä¸ªè¯·æ±‚çš„å¹³å‡ååé‡ï¼ˆtokens/sï¼‰**ï¼šå¹¶å‘è¯·æ±‚çš„å¢åŠ å¯¼è‡´æœåŠ¡å™¨æ¯ä¸ªè¯·æ±‚çš„ååé‡ä¸‹é™ã€‚ä»5ä¸ªå¹¶å‘è¯·æ±‚æ—¶çš„23.21 tokens/sï¼Œä¸‹é™åˆ°20ä¸ªå¹¶å‘è¯·æ±‚æ—¶çš„10.90 tokens/sï¼Œè¿™è¡¨æ˜æœåŠ¡å™¨åœ¨å¤„ç†æ›´å¤šå¹¶å‘è¯·æ±‚æ—¶ï¼Œå¤„ç†æ¯ä¸ªè¯·æ±‚çš„æ•ˆç‡é™ä½ã€‚

3. **å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ç”Ÿæˆtokensçš„é€Ÿåº¦**ï¼šå°½ç®¡æœåŠ¡å™¨çš„å•ä¸ªè¯·æ±‚å¤„ç†é€Ÿåº¦ä¸‹é™ï¼Œä½†éšç€å¹¶å‘è¯·æ±‚çš„å¢åŠ ï¼ŒæœåŠ¡å™¨å’Œå®¢æˆ·ç«¯æ•´ä½“ç”Ÿæˆtokensçš„é€Ÿåº¦éƒ½åœ¨ä¸Šå‡ã€‚è¿™è¡¨æ˜ç³»ç»Ÿåœ¨å¤„ç†æ›´å¤šå¹¶å‘è¯·æ±‚æ—¶ï¼Œæ•´ä½“å¤„ç†èƒ½åŠ›æœ‰æ‰€æé«˜ã€‚

æœ€ä½³å¹¶å‘æ•°çš„ç¡®å®šéœ€è¦æ‰¾åˆ°æœåŠ¡å™¨å¤„ç†è¯·æ±‚æ•ˆç‡å’Œå¹¶å‘æ•°ä¹‹é—´çš„å¹³è¡¡ç‚¹ã€‚ä»ä¸Šè¿°æ•°æ®æ¥çœ‹ï¼Œéšç€å¹¶å‘æ•°çš„å¢åŠ ï¼ŒæœåŠ¡å™¨å•ä¸ªè¯·æ±‚çš„ååé‡ä¸‹é™ï¼Œä½†æ•´ä½“ååé‡ï¼ˆå³æ€»å¤„ç†èƒ½åŠ›ï¼‰åœ¨å¢åŠ ï¼Œç›´åˆ°æŸä¸ªç‚¹åï¼Œå¢åŠ çš„å¹¶å‘æ•°å¯èƒ½å¯¹æœåŠ¡å™¨é€ æˆè¿‡åº¦å‹åŠ›ï¼Œå¯¼è‡´å“åº”æ—¶é—´è¿‡é•¿ï¼Œæ•ˆç‡ä¸‹é™ã€‚åœ¨ç»™å‡ºçš„æ•°æ®ä¸­ï¼Œè¿™ä¸ªå¹³è¡¡ç‚¹å¯èƒ½åœ¨`num_concurrent_requests:10`æ—¶è¾¾åˆ°ï¼Œå› ä¸ºæ­¤æ—¶æœåŠ¡å™¨çš„æ¯ä¸ªè¯·æ±‚ååé‡ï¼ˆ14.93 tokens/sï¼‰å’Œæ•´ä½“ååé‡ï¼ˆ145.34 tokens/sï¼‰éƒ½ç›¸å¯¹è¾ƒé«˜ï¼ŒåŒæ—¶å“åº”æ—¶é—´ï¼ˆ227.48msï¼‰è¿˜åœ¨å¯æ¥å—èŒƒå›´å†…ã€‚ç„¶è€Œï¼Œè¿™éœ€è¦æ ¹æ®å®é™…åº”ç”¨çš„éœ€æ±‚å’ŒæœåŠ¡å™¨çš„æ‰¿è½½èƒ½åŠ›æ¥è¿›ä¸€æ­¥ç¡®è®¤ã€‚
```

## Roadmap

- [] Support streaming inference performance test
- [] Add metric of  error rate